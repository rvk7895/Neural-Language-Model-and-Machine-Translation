{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_spacy = spacy.load('en_core_web_sm')\n",
    "fr_spacy = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 10\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_DIMENSION = 128\n",
    "LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, en_location, fr_location):\n",
    "        self.en_location = en_location\n",
    "        self.fr_location = fr_location\n",
    "        self.corpusSize = 0\n",
    "        self.processed_en_dataset = list()\n",
    "        self.processed_fr_dataset = list()\n",
    "        self.ENvocab = set()\n",
    "        self.ENword2Index = dict()\n",
    "        self.ENindex2Word = list()\n",
    "        self.ENwordFrequency = dict()\n",
    "        self.ENvocabSize = 1\n",
    "        self.FRvocab = set()\n",
    "        self.FRword2Index = dict()\n",
    "        self.FRindex2Word = list()\n",
    "        self.FRwordFrequency = dict()\n",
    "        self.FRvocabSize = 1\n",
    "        self.load_data()\n",
    "        self.preprocessor()\n",
    "        self.vocabBuilder()\n",
    "        self.modifier()\n",
    "        self.combined_data = list()\n",
    "        self.combineData()\n",
    "    \n",
    "    def load_data(self):\n",
    "        with open(self.en_location, 'r') as inFile:\n",
    "            self.en_dataset = inFile.readlines()\n",
    "            self.en_dataset = self.en_dataset[:200000]\n",
    "        \n",
    "        with open(self.fr_location, 'r') as inFile:\n",
    "            self.fr_dataset = inFile.readlines()\n",
    "            self.fr_dataset = self.fr_dataset[:200000]\n",
    "        self.corpusSize = len(self.en_dataset)\n",
    "    \n",
    "    def english_tokenizer(self, text):\n",
    "        return [tok.text for tok in en_spacy.tokenizer(text)]\n",
    "\n",
    "    def french_tokenizer(self, text):\n",
    "        return [tok.text for tok in fr_spacy.tokenizer(text)]\n",
    "\n",
    "    def cleaner(self,sentence):\n",
    "        \"\"\"\n",
    "            replacing !,?,. with . and removing other punctuations\n",
    "            \n",
    "            Arguments:\n",
    "                tokenized corpuse (list)\n",
    "\n",
    "            Returns:\n",
    "                cleaned corpus (list)\n",
    "        \"\"\"\n",
    "        import string\n",
    "\n",
    "        cleaned_corpus = list()\n",
    "\n",
    "        new_sentence = list()\n",
    "        for token in sentence:\n",
    "            if token in string.punctuation or token == '\\n':\n",
    "                continue\n",
    "            else:\n",
    "                new_sentence.append(token)\n",
    "\n",
    "\n",
    "        return new_sentence\n",
    "    \n",
    "    def vocabBuilder(self):\n",
    "        for sentence in self.processed_en_dataset:\n",
    "            for word in sentence:\n",
    "                self.ENvocab.add(word)\n",
    "                if word not in self.ENword2Index:\n",
    "                    self.ENword2Index[word] = self.ENvocabSize\n",
    "                    self.ENindex2Word.append(word)\n",
    "                    self.ENwordFrequency[word] = 1\n",
    "                    self.ENvocabSize += 1\n",
    "                \n",
    "                else:\n",
    "                    self.ENwordFrequency[word] += 1\n",
    "        \n",
    "        for sentence in self.processed_fr_dataset:\n",
    "            for word in sentence:\n",
    "                self.FRvocab.add(word)\n",
    "                if word not in self.FRword2Index:\n",
    "                    self.FRword2Index[word] = self.FRvocabSize\n",
    "                    self.FRindex2Word.append(word)\n",
    "                    self.FRwordFrequency[word] = 1\n",
    "                    self.FRvocabSize += 1\n",
    "                \n",
    "                else:\n",
    "                    self.FRwordFrequency[word] += 1\n",
    "    \n",
    "    def preprocessor(self):\n",
    "        for sentence in self.en_dataset:\n",
    "            tokenized_sentence = self.english_tokenizer(sentence)\n",
    "            cleaned_sentence = self.cleaner(tokenized_sentence)\n",
    "            normalized_sentence = ['<SOS>']\n",
    "            for token in cleaned_sentence:\n",
    "                normalized_sentence.append(token.lower())\n",
    "            normalized_sentence = normalized_sentence + ['<EOS>']\n",
    "\n",
    "            self.processed_en_dataset.append(normalized_sentence)\n",
    "\n",
    "        for sentence in self.fr_dataset:\n",
    "            tokenized_sentence = self.french_tokenizer(sentence)\n",
    "            cleaned_sentence = self.cleaner(tokenized_sentence)\n",
    "            normalized_sentence = ['<SOS>']\n",
    "            for token in cleaned_sentence:\n",
    "                normalized_sentence.append(token.lower())\n",
    "            normalized_sentence = normalized_sentence + ['<EOS>']\n",
    "\n",
    "            self.processed_fr_dataset.append(normalized_sentence)\n",
    "    \n",
    "    def modifier(self):\n",
    "        for i in range(self.corpusSize):\n",
    "            for j in range(1, len(self.processed_en_dataset[i]) - 1):\n",
    "                if self.ENwordFrequency[self.processed_en_dataset[i][j]] < 2:\n",
    "                    self.processed_en_dataset[i][j] = '<OOV>'\n",
    "        \n",
    "        for i in range(self.corpusSize):\n",
    "            for j in range(1, len(self.processed_fr_dataset[i]) - 1):\n",
    "                if self.FRwordFrequency[self.processed_fr_dataset[i][j]] < 2:\n",
    "                    self.processed_fr_dataset[i][j] = '<OOV>'\n",
    "\n",
    "        self.ENvocab = set()\n",
    "        self.ENword2Index = dict()\n",
    "        self.ENindex2Word = list()\n",
    "        self.ENwordFrequency = dict()\n",
    "        self.ENvocabSize = 1\n",
    "        self.FRvocab = set()\n",
    "        self.FRword2Index = dict()\n",
    "        self.FRindex2Word = list()\n",
    "        self.FRwordFrequency = dict()\n",
    "        self.FRvocabSize = 1\n",
    "\n",
    "        for sentence in self.processed_en_dataset:\n",
    "            for word in sentence:\n",
    "                self.ENvocab.add(word)\n",
    "                if word not in self.ENword2Index:\n",
    "                    self.ENword2Index[word] = self.ENvocabSize\n",
    "                    self.ENindex2Word.append(word)\n",
    "                    self.ENwordFrequency[word] = 1\n",
    "                    self.ENvocabSize += 1\n",
    "                \n",
    "                else:\n",
    "                    self.ENwordFrequency[word] += 1\n",
    "        \n",
    "        for sentence in self.processed_fr_dataset:\n",
    "            for word in sentence:\n",
    "                self.FRvocab.add(word)\n",
    "                if word not in self.FRword2Index:\n",
    "                    self.FRword2Index[word] = self.FRvocabSize\n",
    "                    self.FRindex2Word.append(word)\n",
    "                    self.FRwordFrequency[word] = 1\n",
    "                    self.FRvocabSize += 1\n",
    "                \n",
    "                else:\n",
    "                    self.FRwordFrequency[word] += 1\n",
    "    \n",
    "    def combineData(self):\n",
    "        for idx in range(self.corpusSize):\n",
    "            self.combined_data.append((self.processed_en_dataset[idx], self.processed_fr_dataset[idx]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpusSize\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            np.array([self.ENword2Index[word] for word in self.combined_data[index][0]]),\n",
    "            np.array([self.FRword2Index[word] for word in self.combined_data[index][1]])\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, en_file_location, fr_file_location, sequence_length):\n",
    "        self.en_file_location = en_file_location\n",
    "        self.fr_file_location = fr_file_location\n",
    "        self.sequence_length = sequence_length\n",
    "        self.initialize_data()\n",
    "        self.modify()\n",
    "        self.combined_data = list()\n",
    "        self.combineData()\n",
    "\n",
    "    def initialize_data(self):\n",
    "        with open(self.en_file_location, \"r\") as inFile:\n",
    "            enData = inFile.readlines()\n",
    "        \n",
    "        with open(self.fr_file_location, \"r\") as inFile:\n",
    "            frData = inFile.readlines()\n",
    "\n",
    "        en_tokenized_data = self.tokenizer(enData)\n",
    "        self.ENdataset = self.cleaner(en_tokenized_data)\n",
    "        (\n",
    "            self.ENword2Index,\n",
    "            self.ENindex2Word,\n",
    "            self.ENvocab_size,\n",
    "            self.ENvocab,\n",
    "            self.ENwordFrequency\n",
    "        ) = self.vocabBuilder(self.ENdataset)\n",
    "        self.ENwords = list()\n",
    "        for sentence in self.ENdataset:\n",
    "            for word in sentence:\n",
    "                self.ENwords.append(word)\n",
    "\n",
    "        self.ENwords_indexes = [self.ENword2Index[word] for word in self.ENwords]\n",
    "\n",
    "        fr_tokenized_data = self.tokenizer(frData)\n",
    "        self.FRdataset = self.cleaner(fr_tokenized_data)\n",
    "        (\n",
    "            self.FRword2Index,\n",
    "            self.FRindex2Word,\n",
    "            self.FRvocab_size,\n",
    "            self.FRvocab,\n",
    "            self.FRwordFrequency\n",
    "        ) = self.vocabBuilder(self.FRdataset)\n",
    "        self.FRwords = list()\n",
    "        for sentence in self.FRdataset:\n",
    "            for word in sentence:\n",
    "                self.FRwords.append(word)\n",
    "\n",
    "        self.FRwords_indexes = [self.FRword2Index[word] for word in self.FRwords]\n",
    "\n",
    "    def tokenizer(self,corpus):\n",
    "        \"\"\"\n",
    "            tokenizes the corpus\n",
    "            \n",
    "            Arguments:\n",
    "                corpus (list)\n",
    "\n",
    "            Returns:\n",
    "                tokenized corpus (list)\n",
    "        \"\"\"\n",
    "        hashtag_regex = \"#[a-zA-Z0-9]+\"\n",
    "        url_regex = \"((http|https)://)(www.)?[a-zA-Z0-9@:%._\\\\+~#?&//=]{2,256}\\\\.[a-z]{2,6}\\\\b([-a-zA-Z0-9@:%._\\\\+~#?&//=]*)\"\n",
    "        mention_regex = \"@\\w+\"\n",
    "\n",
    "        processed_corpus = list()\n",
    "\n",
    "        for tweet in corpus:\n",
    "            normalized_tweet = tweet.lower()\n",
    "            hashtag_removed_tweet = re.sub(hashtag_regex, \"<HASHTAG>\", normalized_tweet)\n",
    "            website_removed_tweet = re.sub(url_regex, \"<URL>\", hashtag_removed_tweet)\n",
    "            mention_removed_tweet = re.sub(\n",
    "                mention_regex, \"<MENTION>\", website_removed_tweet\n",
    "            )\n",
    "            punctuation_repeat_removed = re.sub(\n",
    "                r\"(\\W)(?=\\1)\", \"\", mention_removed_tweet\n",
    "            )\n",
    "            tokenized_tweet = punctuation_repeat_removed.split()\n",
    "\n",
    "            cleaned_tokenized_tweet = list()\n",
    "            for token in tokenized_tweet:\n",
    "                if token not in [\"<HASHTAG>\", \"<URL>\", \"<MENTION>\", \"<OOV>\"]:\n",
    "                    split_tokens = \"\".join(\n",
    "                        (char if char.isalpha() or char.isnumeric() else f\" {char} \")\n",
    "                        for char in token\n",
    "                    ).split()\n",
    "                    for cleaned_token in split_tokens:\n",
    "                        cleaned_tokenized_tweet.append(cleaned_token)\n",
    "\n",
    "                else:\n",
    "                    cleaned_tokenized_tweet.append(token)\n",
    "            cleaned_tokenized_tweet = ['<SOS>'] + cleaned_tokenized_tweet + ['<EOS>']\n",
    "            processed_corpus.append(cleaned_tokenized_tweet)\n",
    "\n",
    "        return processed_corpus\n",
    "\n",
    "    def cleaner(self,corpus):\n",
    "        \"\"\"\n",
    "            replacing !,?,. with . and removing other punctuations\n",
    "            \n",
    "            Arguments:\n",
    "                tokenized corpuse (list)\n",
    "\n",
    "            Returns:\n",
    "                cleaned corpus (list)\n",
    "        \"\"\"\n",
    "        import string\n",
    "\n",
    "        cleaned_corpus = list()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            new_sentence = list()\n",
    "            for token in sentence:\n",
    "                if token in [\"!\", \".\", \"?\"]:\n",
    "                    new_sentence.append(\".\")\n",
    "                elif token in string.punctuation:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_sentence.append(token)\n",
    "\n",
    "            cleaned_corpus.append(new_sentence)\n",
    "\n",
    "        return cleaned_corpus\n",
    "\n",
    "    def vocabBuilder(self,corpus):\n",
    "        \"\"\"\n",
    "            Builds the vocabulary of the input dataset.\n",
    "\n",
    "            Arguments:\n",
    "                The cleaned tokenized the dataset\n",
    "            \n",
    "            Returns:\n",
    "                Word to Index dict, Index to Word list, Number of Unique Words, Set of Vocab\n",
    "        \"\"\"\n",
    "        word2Index = dict()\n",
    "        index2Word = list()\n",
    "        vocab = set()\n",
    "        wordFrequency = dict()\n",
    "\n",
    "        n_unique_words = 0\n",
    "\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                vocab.add(word)\n",
    "                if word not in word2Index:\n",
    "                    word2Index[word] = n_unique_words\n",
    "                    index2Word.append(word)\n",
    "                    n_unique_words += 1\n",
    "                    wordFrequency[word] = 1\n",
    "                else:\n",
    "                    wordFrequency[word] += 1\n",
    "\n",
    "        return word2Index, index2Word, n_unique_words, vocab, wordFrequency\n",
    "    \n",
    "    def modify(self):\n",
    "        for i in range(len(self.ENdataset)):\n",
    "            for j in range(len(self.ENdataset[i])):\n",
    "                if self.ENwordFrequency[self.ENdataset[i][j]] < 2:\n",
    "                    self.ENdataset[i][j] = '<OOV>'\n",
    "                elif any(character.isdigit() for character in self.ENdataset[i][j]):\n",
    "                    self.ENdataset[i][j] = '<OOV>'\n",
    "\n",
    "        print(self.ENvocab_size)\n",
    "        \n",
    "        self.ENdataset = self.cleaner(self.ENdataset)\n",
    "        (\n",
    "            self.ENword2Index,\n",
    "            self.ENindex2Word,\n",
    "            self.ENvocab_size,\n",
    "            self.ENvocab,\n",
    "            self.ENwordFrequency\n",
    "        ) = self.vocabBuilder(self.ENdataset)\n",
    "        self.ENwords = list()\n",
    "        for sentence in self.ENdataset:\n",
    "            for word in sentence:\n",
    "                self.ENwords.append(word)\n",
    "\n",
    "        self.ENwords_indexes = [self.ENword2Index[word] for word in self.ENwords]\n",
    "\n",
    "        for i in range(len(self.FRdataset)):\n",
    "            for j in range(len(self.FRdataset[i])):\n",
    "                if self.FRwordFrequency[self.FRdataset[i][j]] < 2:\n",
    "                    self.FRdataset[i][j] = '<OOV>'\n",
    "                elif any(character.isdigit() for character in self.FRdataset[i][j]):\n",
    "                    self.FRdataset[i][j] = '<OOV>'\n",
    "\n",
    "        self.FRdataset = self.cleaner(self.FRdataset)\n",
    "        (\n",
    "            self.FRword2Index,\n",
    "            self.FRindex2Word,\n",
    "            self.FRvocab_size,\n",
    "            self.FRvocab,\n",
    "            self.FRwordFrequency\n",
    "        ) = self.vocabBuilder(self.FRdataset)\n",
    "        self.FRwords = list()\n",
    "        for sentence in self.FRdataset:\n",
    "            for word in sentence:\n",
    "                self.FRwords.append(word)\n",
    "\n",
    "        self.FRwords_indexes = [self.FRword2Index[word] for word in self.FRwords]\n",
    "\n",
    "        print(self.FRvocab_size)\n",
    "\n",
    "    def combineData(self):\n",
    "        for idx in range(len(self.ENdataset)):\n",
    "            self.combined_data.append((self.ENdataset[idx], self.FRdataset[idx]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.FRdataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            np.array(self.ENwords_indexes[index : index + self.sequence_length]),\n",
    "            np.array(self.FRwords_indexes[index : index + self.sequence_length])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21589\n",
      "15823\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"./data/ted-talks-corpus/train.en\", \"./data/ted-talks-corpus/train.fr\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.combined_data = sorted(data.combined_data, key=lambda x:len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data):\n",
    "    X = [x[0] for x in data]\n",
    "    Y = [y[1] for y in data]\n",
    "\n",
    "    x_len = max([len(x) for x in X])\n",
    "    y_len = max([len(y) for y in Y])\n",
    "\n",
    "    padded_x = np.zeros((BATCH_SIZE, x_len))\n",
    "    padded_y = np.zeros((BATCH_SIZE, y_len))\n",
    "\n",
    "    for idx, (x, y) in enumerate(zip(X,Y)):\n",
    "        padded_x[idx] = numpy.pad(x, (0,x_len - len(x)))\n",
    "        padded_y[idx] = numpy.pad(y, (0,y_len - len(y))) \n",
    "    \n",
    "    return (\n",
    "        torch.tensor(padded_x, dtype=torch.long).t().to(device),\n",
    "        torch.tensor(padded_y, dtype=torch.long).t().to(device)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, shuffle=False, collate_fn=collate, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout ,output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding_layer = nn.Embedding(self.input_dim, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, source):\n",
    "        embedding = self.dropout(self.embedding_layer(source))\n",
    "        output, (state_h, state_c) = self.lstm(embedding)\n",
    "        return state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_layer = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, source, state_h, state_c):\n",
    "        source = source.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding_layer(source))\n",
    "        output, (state_h, state_c) = self.lstm(embedding, (state_h, state_c))\n",
    "        pred = self.fc(output.squeeze(0))\n",
    "        return pred, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, ground_truth, force_teaching_ratio=0.5):\n",
    "        # ground_truth.shape[0] = lenght of the sentence\n",
    "        # ground_truth.shape[1] = batch_size\n",
    "        state_h, state_c = self.encoder(source)\n",
    "        outputs = torch.zeros(ground_truth.shape[0], ground_truth.shape[1], self.decoder.output_dim).to(device)\n",
    "        decoder_input = ground_truth[0,:]\n",
    "\n",
    "        for idx in range(1,ground_truth.shape[0]):\n",
    "            output, state_h, state_c = self.decoder(decoder_input, state_h, state_c)\n",
    "            outputs[idx] = output\n",
    "            force = random.random() < force_teaching_ratio\n",
    "            predicted = output.argmax(1)\n",
    "            decoder_input = ground_truth[idx] if force else predicted \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataloader):\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train().to(device)\n",
    "        epoch_loss = 0.0\n",
    "        for x,y in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x, y)\n",
    "            pred = pred[1:].reshape(-1,pred.shape[-1])\n",
    "            y = y[1:].reshape(-1)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print({ 'epoch': epoch, 'loss':epoch_loss/len(dataloader) })        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        lstm_size=128,\n",
    "        n_layers=3,\n",
    "        embedding_dim=128,\n",
    "    ):\n",
    "        super(RNN, self).__init__()\n",
    "        self.vocab_size = dataset.vocab_size\n",
    "        self.input_dim = lstm_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_hidden_dim = lstm_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=self.lstm_hidden_dim,\n",
    "            num_layers=self.n_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_hidden_dim, self.vocab_size)\n",
    "        self.output_dim = data.FRvocabSize\n",
    "\n",
    "    def forward(self, x, prev_state=None):\n",
    "        if prev_state == None:\n",
    "            prev_state = self.init_state(1)\n",
    "        embed = self.embedding_layer(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (\n",
    "            torch.zeros(self.n_layers, sequence_length, self.lstm_hidden_dim).to(device),\n",
    "            torch.zeros(self.n_layers, sequence_length, self.lstm_hidden_dim).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(12824,EMBEDDING_SIZE, HIDDEN_DIMENSION, LAYERS, 0.5, 12824)\n",
    "dec = Decoder(15821,EMBEDDING_SIZE, HIDDEN_DIMENSION, LAYERS, 0.5)\n",
    "enc.load_state_dict(torch.load('./models/encoder_weights.pth'))\n",
    "dec.load_state_dict(torch.load('./models/decoder_weights.pth'))\n",
    "model = Seq2Seq(enc, dec)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:15<00:00, 117.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'loss': 7.575274179077148}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:16<00:00, 111.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'loss': 6.118623185602824}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:17<00:00, 106.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'loss': 5.848414396158854}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:17<00:00, 106.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'loss': 5.734406038792928}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 103.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'loss': 5.667627642186483}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 101.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'loss': 5.620791695912679}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 101.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'loss': 5.574699334462483}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 99.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'loss': 5.531569848378499}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 100.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'loss': 5.484510994593302}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 97.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'loss': 5.438163017654419}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './models/MT2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding_layer): Embedding(12824, 128)\n",
      "    (lstm): LSTM(128, 128, num_layers=3, dropout=0.5)\n",
      "    (fc): Linear(in_features=128, out_features=12824, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding_layer): Embedding(15821, 128)\n",
      "    (lstm): LSTM(128, 128, num_layers=3, dropout=0.5)\n",
      "    (fc): Linear(in_features=128, out_features=15821, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    tokens = text.split(' ')\n",
    "    print(tokens)\n",
    "    for idx in range(len(tokens)):\n",
    "      if tokens[idx] not in data.ENvocab:\n",
    "        tokens[idx] = '<OOV>'\n",
    "    \n",
    "    tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "    src_indexes = [data.ENword2Index[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src_tensor = src_tensor.reshape(-1,1)\n",
    "\n",
    "    output = model(src_tensor, src_tensor)\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output.view(-1, output_dim)\n",
    "    indices = torch.argmax(output,dim=1).tolist()\n",
    "    return [data.FRindex2Word[x] for x in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea']\n",
      "['<SOS>', 'la', 'est', '.', '<EOS>', '<SOS>', 'la', 'musique', '.', 'la', 'la', 'ce', 'ce']\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"and we're going to tell you some stories from the sea\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "\n",
    "def calculate_bleu_score(source, target):\n",
    "    translated = translate(source)[1:]\n",
    "    target_tokenized = data.french_tokenizer(target)\n",
    "    score = sentence_bleu([target_tokenized], translated, weights=(0.75,0.25,0,0))\n",
    "    return score, translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'david', 'gallo', 'this', 'is', 'bill', 'lange', '.', 'i', 'm', 'dave', 'gallo', '.', '<EOS>']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'french_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_148390/450911448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFRdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcandidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrench_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_bleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'french_tokenizer'"
     ]
    }
   ],
   "source": [
    "total_score = 0\n",
    "candidates, references = list(), list()\n",
    "scores = []\n",
    "for source, target in zip(data.ENdataset, data.FRdataset):\n",
    "    candidates.append(translate(\" \".join(source))[1:])\n",
    "    references.append([target.split(\" \")])\n",
    "    score, translated = calculate_bleu_score(source, target)\n",
    "    total_score += score\n",
    "    scores.append(f'{\" \".join(translated)}\\t{score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "Fraction(0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_148390/4128043330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Collects the various precision values for the different ngram orders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     p_n = [\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mFraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_numerators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_denominators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Collects the various precision values for the different ngram orders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     p_n = [\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mFraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_numerators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_denominators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/fractions.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, numerator, denominator, _normalize)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdenominator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fraction(%s, 0)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnumerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_normalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Fraction(0, 0)"
     ]
    }
   ],
   "source": [
    "print(corpus_bleu(references, candidates, weights=(1,0,0,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008333777127709976\n"
     ]
    }
   ],
   "source": [
    "print(total_score/20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = open('./bleu_scores/2019115002_MT_train_scores.txt', 'a')\n",
    "\n",
    "for line in scores:\n",
    "    outFile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ted-talks-corpus/test.en', 'r') as inFile:\n",
    "    en_dataset = inFile.readlines()\n",
    "        \n",
    "with open('./data/ted-talks-corpus/test.fr', 'r') as inFile:\n",
    "    fr_dataset = inFile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score = 0\n",
    "candidates, references = list(), list()\n",
    "scores = []\n",
    "for source, target in zip(en_dataset, fr_dataset):\n",
    "    try:\n",
    "        candidates.append(translate(source)[1:])\n",
    "        references.append([data.french_tokenizer(target)])\n",
    "        score, translated = calculate_bleu_score(source, target)\n",
    "        total_score += score\n",
    "        scores.append(f'{\" \".join(translated)}\\t{score}\\n')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03098522378317546\n"
     ]
    }
   ],
   "source": [
    "print(corpus_bleu(references, candidates, weights=(1,0,0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = open('./bleu_scores/2019115002_MT_test_scores.txt', 'a')\n",
    "\n",
    "for line in scores:\n",
    "    outFile.write(line)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f54ec556731191ae34ffecbd12704bb8f2e6a5cabce98c16cdcccd50acdfe5bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
